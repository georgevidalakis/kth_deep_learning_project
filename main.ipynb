{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzA4wZLlOZJQ",
        "outputId": "d18ee9a2-19a5-4490-d8d6-bbc463deed36"
      },
      "outputs": [],
      "source": [
        "# !unzip drive/MyDrive/Pets-data/images.zip\n",
        "# !cp -r drive/MyDrive/Pets-data/annotations .\n",
        "# !ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "G05VSG-WExN-"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import yaml\n",
        "from typing import Tuple, List\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "from pydantic import BaseModel\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "mw2iUZkZXMW3"
      },
      "outputs": [],
      "source": [
        "# Define constants\n",
        "\n",
        "CONFIG_FILE_PATH = 'config.yaml'\n",
        "IMAGE_COL_IDX = 0\n",
        "CLASS_ID_COL_IDX = 1\n",
        "SPECIES_COL_IDX = 2\n",
        "POSSIBLE_NUM_CLASSES = {2, 37}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "V_BHkWLlXMW3"
      },
      "outputs": [],
      "source": [
        "class AdamOptimizerConfig(BaseModel):\n",
        "    lr: float\n",
        "    weight_decay: float"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XkhfRZxKXMW4"
      },
      "outputs": [],
      "source": [
        "class Config(BaseModel):\n",
        "    device: str\n",
        "    num_classes: int\n",
        "    batch_size: int\n",
        "    max_num_epochs: int\n",
        "    patience: int\n",
        "    adam_optimizer_config: AdamOptimizerConfig\n",
        "    num_batch_norm_layers_to_train_params: int\n",
        "    num_batch_norm_layers_to_update_running_stats: int\n",
        "    train_earlier_layers_delay: int\n",
        "    n_hidden_layers_to_train: int"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4nXmlOeyXMW4"
      },
      "outputs": [],
      "source": [
        "with open(CONFIG_FILE_PATH, encoding='utf-8') as f:\n",
        "    config_dict = yaml.load(f, Loader=yaml.FullLoader)\n",
        "\n",
        "config = Config.model_validate(config_dict)\n",
        "\n",
        "assert config.num_classes in POSSIBLE_NUM_CLASSES\n",
        "assert 0 <= config.num_batch_norm_layers_to_train_params <= 36  # 36 batch norm layers in resnet34\n",
        "assert 0 <= config.num_batch_norm_layers_to_update_running_stats <= 36  # 36 batch norm layers in resnet34"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3bs_rvb7HsqY"
      },
      "outputs": [],
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, filenames: List[str], labels: List[int], use_augmentations: bool) -> None:\n",
        "        self.filenames = filenames\n",
        "        self.labels = labels\n",
        "        # self.transformation = torchvision.models.ResNet34_Weights.IMAGENET1K_V1.transforms()\n",
        "        self.transformation = (\n",
        "            transforms.Compose([\n",
        "                torchvision.models.ResNet34_Weights.IMAGENET1K_V1.transforms(),\n",
        "                # transforms.RandomCrop((what size, what other size)),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                # transforms.RandomRotation((-10, 10)),\n",
        "                # transforms.RandomErasing(),\n",
        "            ])\n",
        "            if use_augmentations\n",
        "            else torchvision.models.ResNet34_Weights.IMAGENET1K_V1.transforms()\n",
        "        )\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.filenames)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        image = Image.open(os.path.join('images', f'{self.filenames[idx]}.jpg')).convert('RGB')\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        transformed_img = self.transformation(image)\n",
        "\n",
        "        return transformed_img.to(config.device), torch.tensor(label).to(config.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VWB1d993Kmmp"
      },
      "outputs": [],
      "source": [
        "def get_image_names_and_labels(annotations_file_path: str, num_classes: int) -> Tuple[List[str], List[int]]:\n",
        "    filenames: List[str] = []\n",
        "    labels: List[int] = []\n",
        "\n",
        "    with open(annotations_file_path, encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "\n",
        "    label_col_idx = SPECIES_COL_IDX if num_classes == 2 else CLASS_ID_COL_IDX\n",
        "\n",
        "    for line in lines:\n",
        "        line_split = line.split()\n",
        "        filenames.append(line_split[IMAGE_COL_IDX])\n",
        "        labels.append(int(line_split[label_col_idx]) - 1)\n",
        "\n",
        "    return filenames, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SWMeQTkAXMW5"
      },
      "outputs": [],
      "source": [
        "def get_batch_norm_layers(model: nn.Module) -> List[nn.Module]:\n",
        "    return [\n",
        "        module\n",
        "        for module in model.modules()\n",
        "        if isinstance(module, nn.BatchNorm2d)\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "nopF1mMoExN_"
      },
      "outputs": [],
      "source": [
        "def get_pretrained_model_with_trainable_last_layer(num_batch_norm_layers_to_train_params: int) -> nn.Module:\n",
        "    model = torchvision.models.resnet34(weights=torchvision.models.ResNet34_Weights.IMAGENET1K_V1)\n",
        "\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    batch_norm_layers = get_batch_norm_layers(model)\n",
        "    batch_norm_layers_to_train_params = batch_norm_layers[-num_batch_norm_layers_to_train_params:] if num_batch_norm_layers_to_train_params else []\n",
        "\n",
        "    for batch_norm_layer in batch_norm_layers_to_train_params:\n",
        "        for param in batch_norm_layer.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    model.fc = nn.Linear(in_features=model.fc.in_features, out_features=config.num_classes)\n",
        "\n",
        "    return model.to(config.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "FBNnljpQZdTp"
      },
      "outputs": [],
      "source": [
        "def get_pretrained_model_with_trainable_n_layers(num_batch_norm_layers_to_train_params: int, number_of_hidden_layers_to_train: int=0) -> nn.Module:\n",
        "    model = torchvision.models.resnet34(weights=torchvision.models.ResNet34_Weights.IMAGENET1K_V1)\n",
        "\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    batch_norm_layers = get_batch_norm_layers(model)\n",
        "    batch_norm_layers_to_train_params = batch_norm_layers[-num_batch_norm_layers_to_train_params:] if num_batch_norm_layers_to_train_params else []\n",
        "\n",
        "    for batch_norm_layer in batch_norm_layers_to_train_params:\n",
        "        for param in batch_norm_layer.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    model.fc = nn.Linear(in_features=model.fc.in_features, out_features=config.num_classes)\n",
        "\n",
        "    trainable_layers = [layer for layer in model.modules() if not isinstance(layer, torchvision.models.resnet.ResNet) and not isinstance(layer, torchvision.models.resnet.BasicBlock) and not isinstance(layer, nn.Sequential) and not isinstance(layer, nn.Sequential) and len(list(layer.parameters())) > 0]\n",
        "    trainable_layers = list(reversed(trainable_layers[:-1]))\n",
        "\n",
        "    for l in trainable_layers[:number_of_hidden_layers_to_train]:\n",
        "        for p in l.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "    return model.to(config.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Xx6TYx-30Gy7"
      },
      "outputs": [],
      "source": [
        "def make_hidden_layers_trainable(model: nn.Module, number_of_hidden_layers_to_train: int) -> nn.Module:\n",
        "    trainable_layers = [layer for layer in model.modules() if not isinstance(layer, torchvision.models.resnet.ResNet) and not isinstance(layer, torchvision.models.resnet.BasicBlock) and not isinstance(layer, nn.Sequential) and not isinstance(layer, nn.Sequential) and len(list(layer.parameters())) > 0]\n",
        "    trainable_layers = list(reversed(trainable_layers[:-1]))\n",
        "\n",
        "    for l in trainable_layers[:number_of_hidden_layers_to_train]:\n",
        "        for p in l.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "PPeg-qfgXMW6"
      },
      "outputs": [],
      "source": [
        "def get_model_accuracy(model: nn.Module, data_loader: DataLoader) -> float:\n",
        "    correct_predictions_cnt = 0\n",
        "    total_predictions_cnt = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in tqdm(data_loader, desc='Computing accuracy'):\n",
        "            outputs = model(inputs)\n",
        "            correct_predictions_cnt += (torch.argmax(outputs, axis=1) == labels).sum()\n",
        "            total_predictions_cnt += len(outputs)\n",
        "    return correct_predictions_cnt / total_predictions_cnt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ij6jIhTVXMW6"
      },
      "outputs": [],
      "source": [
        "def train_single_epoch(\n",
        "        model: nn.Module,\n",
        "        train_data_loader: DataLoader,\n",
        "        criterion: nn.Module,\n",
        "        optimizer: torch.optim.Optimizer,\n",
        "        num_batch_norm_layers_to_update_running_stats: int,\n",
        "        ) -> float:\n",
        "    model.train()\n",
        "    batch_norm_layers = get_batch_norm_layers(model)\n",
        "    batch_norm_layers_to_not_update_running_stats = (\n",
        "        batch_norm_layers[:-num_batch_norm_layers_to_update_running_stats]\n",
        "        if num_batch_norm_layers_to_update_running_stats\n",
        "        else batch_norm_layers\n",
        "    )\n",
        "    for batch_norm_layer in batch_norm_layers_to_not_update_running_stats:\n",
        "        batch_norm_layer.eval()\n",
        "    train_loss_sum = 0.0\n",
        "    train_samples_cnt = 0\n",
        "    for inputs, labels in tqdm(train_data_loader, desc='Training model'):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss_sum += loss.item() * len(outputs)\n",
        "        train_samples_cnt += len(outputs)\n",
        "    return train_loss_sum / train_samples_cnt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "XIdYAijVXMW7"
      },
      "outputs": [],
      "source": [
        "def save_checkpoint(model: nn.Module, optimizer: torch.optim.Optimizer) -> Tuple[nn.Module, torch.optim.Optimizer]:\n",
        "    checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "    }\n",
        "    torch.save(checkpoint, 'checkpoint.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "puyQaj1KXMW7"
      },
      "outputs": [],
      "source": [
        "def load_checkpoint(model: nn.Module, optimizer: torch.optim.Optimizer) -> None:\n",
        "    checkpoint = torch.load('checkpoint.pt')\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "l_Z-eIVoExOB"
      },
      "outputs": [],
      "source": [
        "filenames_trainval, labels_trainval = get_image_names_and_labels('annotations/trainval.txt', num_classes=config.num_classes)\n",
        "filenames_test, labels_test = get_image_names_and_labels('annotations/test.txt', num_classes=config.num_classes)\n",
        "filenames_train, filenames_val, labels_train, labels_val = train_test_split(filenames_trainval, labels_trainval, test_size=0.2, stratify=labels_trainval)\n",
        "\n",
        "dataset_train = ImageDataset(filenames_train, labels_train, use_augmentations=True)\n",
        "dataset_val = ImageDataset(filenames_val, labels_val, use_augmentations=False)\n",
        "dataset_test = ImageDataset(filenames_test, labels_test, use_augmentations=False)\n",
        "\n",
        "train_data_loader = DataLoader(dataset_train, batch_size=config.batch_size, shuffle=True)\n",
        "val_data_loader = DataLoader(dataset_val, batch_size=config.batch_size, shuffle=False)\n",
        "test_data_loader = DataLoader(dataset_test, batch_size=config.batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYVA1JYkN0ZB",
        "outputId": "d0ab9637-378d-402f-d24e-a94f19ff11c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch #0:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training model: 100%|██████████| 92/92 [00:12<00:00,  7.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 1.6885751090619876\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing accuracy: 100%|██████████| 23/23 [00:02<00:00,  7.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy: 88.18% (new best)\n",
            "Checkpoint saved\n",
            "\n",
            "Epoch #1:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training model: 100%|██████████| 92/92 [00:12<00:00,  7.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.5084084165485009\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing accuracy: 100%|██████████| 23/23 [00:02<00:00,  8.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy: 91.85% (new best)\n",
            "Checkpoint saved\n",
            "\n",
            "Epoch #2:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training model: 100%|██████████| 92/92 [00:12<00:00,  7.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.33681971288245655\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing accuracy: 100%|██████████| 23/23 [00:02<00:00,  8.19it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy: 91.58% (worse than 91.85% of epoch 1)\n",
            "\n",
            "Epoch #3:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training model: 100%|██████████| 92/92 [00:12<00:00,  7.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.25884414504727593\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing accuracy: 100%|██████████| 23/23 [00:02<00:00,  8.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy: 93.48% (new best)\n",
            "Checkpoint saved\n",
            "\n",
            "Epoch #4:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training model: 100%|██████████| 92/92 [00:12<00:00,  7.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.20880886062007883\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing accuracy: 100%|██████████| 23/23 [00:02<00:00,  8.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy: 93.34% (worse than 93.48% of epoch 3)\n",
            "\n",
            "Epoch #5:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training model: 100%|██████████| 92/92 [00:12<00:00,  7.58it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.17626360577085745\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing accuracy: 100%|██████████| 23/23 [00:02<00:00,  8.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy: 93.61% (new best)\n",
            "Checkpoint saved\n",
            "Making last 3 hidden layers trainable\n",
            "\n",
            "Epoch #6:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training model: 100%|██████████| 92/92 [00:12<00:00,  7.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.14512542906500722\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing accuracy: 100%|██████████| 23/23 [00:02<00:00,  8.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy: 93.61% (worse than 93.61% of epoch 5)\n",
            "\n",
            "Epoch #7:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training model: 100%|██████████| 92/92 [00:12<00:00,  7.54it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.1267847473449681\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing accuracy: 100%|██████████| 23/23 [00:02<00:00,  8.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy: 93.48% (worse than 93.61% of epoch 5)\n",
            "\n",
            "Epoch #8:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training model: 100%|██████████| 92/92 [00:12<00:00,  7.57it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.11769688429067963\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing accuracy: 100%|██████████| 23/23 [00:02<00:00,  8.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy: 93.48% (worse than 93.61% of epoch 5)\n",
            "\n",
            "Epoch #9:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training model: 100%|██████████| 92/92 [00:12<00:00,  7.53it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.10921055382198613\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing accuracy: 100%|██████████| 23/23 [00:02<00:00,  8.32it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy: 93.61% (worse than 93.61% of epoch 5)\n",
            "\n",
            "Epoch #10:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training model: 100%|██████████| 92/92 [00:12<00:00,  7.34it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.09937814983498791\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing accuracy: 100%|██████████| 23/23 [00:02<00:00,  8.14it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy: 94.29% (new best)\n",
            "Checkpoint saved\n",
            "\n",
            "Epoch #11:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training model: 100%|██████████| 92/92 [00:12<00:00,  7.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.09259062867773615\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing accuracy: 100%|██████████| 23/23 [00:02<00:00,  8.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy: 94.16% (worse than 94.29% of epoch 10)\n",
            "\n",
            "Epoch #12:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training model: 100%|██████████| 92/92 [00:12<00:00,  7.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.08814183286512675\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing accuracy: 100%|██████████| 23/23 [00:02<00:00,  8.01it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy: 94.16% (worse than 94.29% of epoch 10)\n",
            "\n",
            "Epoch #13:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training model: 100%|██████████| 92/92 [00:12<00:00,  7.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.08415749513179711\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing accuracy: 100%|██████████| 23/23 [00:02<00:00,  7.96it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy: 93.61% (worse than 94.29% of epoch 10)\n",
            "\n",
            "Epoch #14:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training model: 100%|██████████| 92/92 [00:12<00:00,  7.47it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.07949819259912423\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing accuracy: 100%|██████████| 23/23 [00:02<00:00,  8.28it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy: 93.61% (worse than 94.29% of epoch 10)\n",
            "\n",
            "Epoch #15:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training model: 100%|██████████| 92/92 [00:12<00:00,  7.44it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.07381837369631165\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing accuracy: 100%|██████████| 23/23 [00:02<00:00,  8.13it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy: 93.89% (worse than 94.29% of epoch 10)\n",
            "\n",
            "Epoch #16:\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training model: 100%|██████████| 92/92 [00:12<00:00,  7.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.07130614143755773\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing accuracy: 100%|██████████| 23/23 [00:02<00:00,  8.05it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation accuracy: 93.07% (worse than 94.29% of epoch 10)\n",
            "Early stopping\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "model = get_pretrained_model_with_trainable_last_layer(config.num_batch_norm_layers_to_train_params)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=config.adam_optimizer_config.lr, weight_decay=config.adam_optimizer_config.weight_decay)\n",
        "\n",
        "max_val_accuracy = float('-inf')\n",
        "argmax_epoch = -1\n",
        "\n",
        "for epoch in range(config.max_num_epochs):\n",
        "    print(f'Epoch #{epoch}:')\n",
        "    train_loss = train_single_epoch(model, train_data_loader, criterion, optimizer, config.num_batch_norm_layers_to_update_running_stats)\n",
        "    print(f'Train loss: {train_loss}')\n",
        "    val_accuracy = get_model_accuracy(model, val_data_loader)\n",
        "    if val_accuracy > max_val_accuracy:\n",
        "        print(f'Validation accuracy: {100 * val_accuracy:.2f}% (new best)')\n",
        "        max_val_accuracy = val_accuracy\n",
        "        argmax_epoch = epoch\n",
        "        save_checkpoint(model, optimizer)\n",
        "        print('Checkpoint saved')\n",
        "    else:\n",
        "        print(f'Validation accuracy: {100 * val_accuracy:.2f}% (worse than {100 * max_val_accuracy:.2f}% of epoch {argmax_epoch})')\n",
        "        if epoch > argmax_epoch + config.patience:\n",
        "            print(f'Early stopping')\n",
        "            break\n",
        "    # Start training earlier layers\n",
        "    if epoch == config.train_earlier_layers_delay and config.n_hidden_layers_to_train > 0:\n",
        "        print (f'Making last {config.n_hidden_layers_to_train} hidden layers trainable')\n",
        "        make_hidden_layers_trainable(model, config.n_hidden_layers_to_train)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=config.adam_optimizer_config.lr / 100, weight_decay=config.adam_optimizer_config.weight_decay)\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vR85iuaLXMW9",
        "outputId": "0f984362-01e7-47f8-cdb4-310d5ef63e17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint loaded\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Computing accuracy: 100%|██████████| 115/115 [00:15<00:00,  7.57it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 91.44%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "load_checkpoint(model, optimizer)\n",
        "print('Checkpoint loaded')\n",
        "\n",
        "test_accuracy = get_model_accuracy(model, test_data_loader)\n",
        "\n",
        "print(f'Test accuracy: {100 * test_accuracy:.2f}%')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
